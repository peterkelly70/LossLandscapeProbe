import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
import torchvision.models as models
import numpy as np
import logging
from pathlib import Path
import json
import os
import time
import random
from typing import Dict, List, Tuple, Any, Optional, Union, Callable
import matplotlib.pyplot as plt
from sklearn.model_selection import ParameterGrid
from torch.utils.data import DataLoader, Subset
from torchvision import datasets, transforms
from .utils import create_model, create_optimizer, set_seed, get_device
from .data_utils import get_cifar_data, create_data_subsets

logger = logging.getLogger(__name__)

class CIFARMetaModelOptimizer:
    """
    Meta-model optimizer for CIFAR datasets.
    
    This class handles the meta-model training, hyperparameter prediction,
    and evaluation on CIFAR datasets.
    """
    
    def __init__(self, config):
        """
        Initialize the meta-model optimizer.
        
        Args:
            config: Configuration dictionary with parameters
        """
        self.config = config
        set_seed(config.seed)
        self.device = get_device()
        
        # Load dataset
        self.train_loader, self.val_loader, self.test_loader = get_cifar_data(
            dataset=config.dataset,
            batch_size=config.batch_size,
            val_split=config.val_split,
            num_workers=config.num_workers
        )
        
        # Set number of classes based on dataset
        self.num_classes = 10 if config.dataset == 'cifar10' else 100
        
        # Create data subsets for efficient evaluation if specified
        if hasattr(config, 'num_subsets') and config.num_subsets > 0:
            self.data_subsets = create_data_subsets(
                self.train_loader.dataset,
                num_subsets=config.num_subsets,
                subset_size=config.subset_size
            )
            logger.info(f"Created {len(self.data_subsets)} data subsets with {config.subset_size} samples each")
        
        # Initialize hyperparameter search space
        self._init_search_space()
        
        # Path for saving meta-model
        self.meta_model_path = Path(config.output_dir) / "meta_model.pt"
        
        # Initialize meta-model
        self.meta_model = None
        self.meta_model_trained = False
        
        logger.info(f"Initialized CIFARMetaModelOptimizer with {config.dataset} dataset")
        logger.info(f"Device: {self.device}")
        
    def _init_search_space(self):
        """Initialize the hyperparameter search space."""
        # Define the hyperparameter search space
        self.param_grid = {
            'learning_rate': [0.1, 0.01, 0.001],
            'optimizer': ['sgd', 'adam'],
            'batch_size': [64, 128, 256],
            'num_channels': [32, 64],
            'dropout_rate': [0.3, 0.5],
            'weight_decay': [1e-4, 1e-5],
            'use_lr_scheduler': [True, False]
        }
        
        # Optional parameters for specific optimizers or configurations
        self.conditional_params = {
            'sgd': {'momentum': [0.9, 0.95]},
            'use_lr_scheduler': {
                True: {
                    'lr_decay_factor': [0.1, 0.2],
                    'lr_patience': [5, 10]
                }
            }
        }
        
        # Generate all valid hyperparameter configurations
        self.all_configs = list(ParameterGrid(self.param_grid))
        
        # Add conditional parameters
        for config in self.all_configs:
            # Add optimizer-specific parameters
            if config['optimizer'] in self.conditional_params:
                for param, values in self.conditional_params[config['optimizer']].items():
                    config[param] = random.choice(values)
            
            # Add scheduler-specific parameters
            if config['use_lr_scheduler'] and 'use_lr_scheduler' in self.conditional_params:
                for param, values in self.conditional_params['use_lr_scheduler'][True].items():
                    config[param] = random.choice(values)
        
        logger.info(f"Initialized search space with {len(self.all_configs)} configurations")
    
    def _get_subset_loader(self, subset_idx, batch_size=None):
        """
        Get a data loader for a specific subset.
        
        Args:
            subset_idx: Index of the subset to use
            batch_size: Batch size for the data loader
            
        Returns:
            DataLoader for the specified subset
        """
        if not hasattr(self, 'data_subsets'):
            raise ValueError("Data subsets not initialized")
        
        if subset_idx < 0 or subset_idx >= len(self.data_subsets):
            raise ValueError(f"Invalid subset index: {subset_idx}")
        
        if batch_size is None:
            batch_size = self.config.batch_size
        
        subset = self.data_subsets[subset_idx]
        return torch.utils.data.DataLoader(
            subset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=2,
            pin_memory=torch.cuda.is_available()
        )
    
    def _evaluate_configuration(
            self, 
            config: Dict[str, Any], 
            resource_level: float = 1.0,  # Now using full resource by default
            epochs: Optional[int] = None,  # Will be taken from config if None
            subset_idx: Optional[int] = None,
            is_perturbation: bool = False
        ) -> Dict[str, Any]:
        """
        Evaluate a single hyperparameter configuration.
        
        Args:
            config: Hyperparameter configuration
            resource_level: Fraction of epochs to use (0.0 to 1.0)
            epochs: Maximum number of epochs (if None, uses config['max_epochs'])
            subset_idx: Optional index of the data subset to use. If None, uses full training set.
            is_perturbation: Whether this is a perturbation evaluation (affects logging)
            
        Returns:
            Dictionary with evaluation results
        """
        # Initialize data_info early to ensure it's always defined
        data_info = "full training set" if subset_idx is None else f"subset {subset_idx+1}/{len(self.data_subsets) if hasattr(self, 'data_subsets') else '?'}"
        eval_type = "perturbation" if is_perturbation else "configuration"
        
        try:
            # Get batch size from config or use default
            batch_size = config.get('batch_size', self.config.batch_size)
            
            # Get max epochs from config if not provided
            max_epochs = epochs if epochs is not None else config.get('max_epochs', 10)
            
            # Calculate actual epochs based on resource level
            actual_epochs = max(1, int(max_epochs * resource_level))
            
            # Get the appropriate data loader with the specified batch size
            if subset_idx is not None:
                train_loader = self._get_subset_loader(subset_idx, batch_size=batch_size)
            else:
                # Create a new loader with the specified batch size for the full dataset
                train_loader = torch.utils.data.DataLoader(
                    self.train_loader.dataset,
                    batch_size=batch_size,
                    shuffle=True,
                    num_workers=2,
                    pin_memory=torch.cuda.is_available()
                )
            
            # Log evaluation start with subset progress percentage if applicable
            if subset_idx is not None and hasattr(self, 'data_subsets'):
                total_subsets = len(self.data_subsets)
                subset_progress = (subset_idx / total_subsets) * 100
                logger.info(f"Evaluating {eval_type} on {data_info} for {actual_epochs} epochs "
                         f"(batch_size={batch_size}, lr={config.get('learning_rate', 0.001):.4f})... "
                         f"[Overall Progress: {subset_progress:.1f}%]")
            else:
                logger.info(f"Evaluating {eval_type} on {data_info} for {actual_epochs} epochs "
                         f"(batch_size={batch_size}, lr={config.get('learning_rate', 0.001):.4f})...")
            
            # Create model and move to device
            model_config = {
                'num_channels': config.get('num_channels', 32),  # Default to 32 if not specified
                'dropout_rate': config.get('dropout_rate', 0.5)  # Default to 0.5 if not specified
            }
            
            # Create model
            model = create_model(
                dataset=self.config.dataset,
                num_classes=self.num_classes,
                **model_config
            ).to(self.device)
            
            # Create optimizer
            optimizer = create_optimizer(
                model=model,
                optimizer_type=config.get('optimizer', 'adam'),
                learning_rate=config.get('learning_rate', 0.001),
                momentum=config.get('momentum', 0.9),
                weight_decay=config.get('weight_decay', 1e-4)
            )
            
            # Create scheduler if specified
            scheduler = None
            if config.get('use_lr_scheduler', False):
                scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                    optimizer,
                    mode='max',
                    factor=config.get('lr_decay_factor', 0.1),
                    patience=config.get('lr_patience', 5),
                    verbose=True
                )
            
            # Training loop
            model.train()
            best_val_accuracy = 0.0
            train_losses = []
            val_accuracies = []
            best_model_state = None
            
            # Track overall progress
            total_epochs = actual_epochs
            total_subsets = len(self.data_subsets) if hasattr(self, 'data_subsets') else 1
            current_subset = subset_idx + 1 if subset_idx is not None else 0
            subset_progress = current_subset / total_subsets if total_subsets > 0 else 0
            
            # Log memory usage if using CUDA
            if self.device.type == 'cuda':
                logger.info(f"Starting training on {torch.cuda.get_device_name(0)}")
                logger.info(f"Initial CUDA memory allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB")
            
            # Epoch loop
            for epoch in range(actual_epochs):
                # Training
                model.train()
                running_loss = 0.0
                total = 0
                processed = 0
                
                for batch_idx, (inputs, targets) in enumerate(train_loader):
                    # Move data to device
                    inputs, targets = inputs.to(self.device), targets.to(self.device)
                    
                    # Zero the parameter gradients
                    optimizer.zero_grad()
                    
                    # Forward pass
                    outputs = model(inputs)
                    loss = F.cross_entropy(outputs, targets)
                    
                    # Backward pass and optimize
                    loss.backward()
                    optimizer.step()
                    
                    # Update statistics
                    running_loss += loss.item() * inputs.size(0)
                    total += targets.size(0)
                    processed += 1
                    
                    # Log progress every 20 batches
                    if batch_idx % 20 == 0 and batch_idx > 0:
                        avg_loss = running_loss / total
                        progress = batch_idx / len(train_loader) * 100
                        logger.debug(f"Epoch {epoch+1}/{actual_epochs} - Batch {batch_idx}/{len(train_loader)} "
                                   f"[{progress:.1f}%] - Loss: {avg_loss:.4f}")
                
                # Calculate average training loss for this epoch
                train_loss = running_loss / total
                train_losses.append(train_loss)
                
                # Validation
                model.eval()
                val_loss = 0.0
                val_correct = 0
                val_total = 0
                
                with torch.no_grad():
                    for batch_idx, (inputs, targets) in enumerate(self.val_loader):
                        # Move data to device
                        inputs, targets = inputs.to(self.device), targets.to(self.device)
                        
                        # Forward pass
                        outputs = model(inputs)
                        loss = F.cross_entropy(outputs, targets)
                        
                        # Update statistics
                        val_loss += loss.item() * inputs.size(0)
                        _, predicted = outputs.max(1)
                        val_total += targets.size(0)
                        val_correct += predicted.eq(targets).sum().item()
                
                # Calculate validation metrics
                val_loss = val_loss / val_total
                val_accuracy = val_correct / val_total
                val_accuracies.append(val_accuracy)
                
                # Update learning rate scheduler if used
                if scheduler is not None:
                    scheduler.step(val_accuracy)
                
                # Save best model
                if val_accuracy > best_val_accuracy:
                    best_val_accuracy = val_accuracy
                    best_model_state = model.state_dict()
                
                # Log progress
                if (epoch + 1) % max(1, actual_epochs // 5) == 0 or (epoch + 1) == actual_epochs:
                    # Calculate overall progress percentage
                    epoch_progress = (epoch + 1) / actual_epochs
                    if subset_idx is not None and hasattr(self, 'data_subsets'):
                        total_subsets = len(self.data_subsets)
                        subset_progress = subset_idx / total_subsets
                        overall_progress = (subset_progress + epoch_progress / total_subsets) * 100
                    else:
                        overall_progress = epoch_progress * 100
                    
                    logger.info(f"Epoch {epoch+1}/{actual_epochs} - "
                             f"Train Loss: {train_loss:.4f}, "
                             f"Val Loss: {val_loss:.4f}, "
                             f"Val Acc: {val_accuracy:.4f} "
                             f"[Overall Progress: {overall_progress:.1f}%]")
            
            # Final evaluation results
            results = {
                'val_accuracy': best_val_accuracy,
                'train_losses': train_losses,
                'val_accuracies': val_accuracies,
                'model_state': best_model_state,
                'config': config,
                'epochs_trained': actual_epochs,
                'resource_level': resource_level
            }
            
            # Log final results
            logger.info(f"Evaluation complete - Best validation accuracy: {best_val_accuracy:.4f}")
            
            return results
            
        except Exception as e:
            logger.error(f"Error during evaluation: {str(e)}")
            raise
